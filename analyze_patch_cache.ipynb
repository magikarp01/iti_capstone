{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the post-patching results in depth\n",
    "We want to probe the activations post activation patching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3401334/1433250938.py:6: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"load_ext autoreload\")\n",
      "/tmp/ipykernel_3401334/1433250938.py:7: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"autoreload 2\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#%%\n",
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "# Code to automatically update the TransformerLens code as its edited without restarting the kernel\n",
    "ipython.magic(\"load_ext autoreload\")\n",
    "ipython.magic(\"autoreload 2\")\n",
    "    \n",
    "import plotly.io as pio\n",
    "# pio.renderers.default = \"png\"\n",
    "# Import stuff\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from jaxtyping import Float, Int\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from utils.probing_utils import ModelActs\n",
    "from utils.dataset_utils import CounterFact_Dataset, TQA_MC_Dataset, EZ_Dataset\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "\n",
    "from utils.iti_utils import patch_iti\n",
    "\n",
    "from utils.analytics_utils import plot_probe_accuracies, plot_norm_diffs, plot_cosine_sims\n",
    "import os\n",
    "from torch import Tensor\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from utils.analytics_utils import plot_z_probe_accuracies, plot_resid_probe_accuracies, acc_tensor_from_dict, get_px_fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format raw activations\n",
    "\n",
    "Format unformatted activation files with each prompt being a single file, into formatted activations of each component's collated acts.\n",
    "\n",
    "Only has to be done once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = 10\n",
    "# N = 2550 #upper bound the global (level 0) index\n",
    "d_head = 128\n",
    "n_layers = 80\n",
    "n_heads = 64\n",
    "patch_id = 1\n",
    "# num_params = \"70b\"\n",
    "\n",
    "from utils.cache_utils import create_probe_dataset, create_all_probe_datasets, format_logits\n",
    "act_type = \"z\"\n",
    "data_dir = \"/mnt/ssd-2/phillipguo3/patching_acts\"\n",
    "splits  =[\"azaria_mitchell_facts\"]\n",
    "# create_probe_dataset(run_id, -1, \"honest\", act_type, data_dir=data_dir, patch_id=patch_id, splits=splits)\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset_name = \"notrichardren/truthfulness_high_quality\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "dataset_indices = [row['ind'] for row in dataset[\"combined\"] if row['dataset'] == \"azaria_mitchell_facts\"][:300]\n",
    "\n",
    "# format_logits(dataset_indices, \"azaria_mitchell_facts\", f\"{data_dir}/data/large_run_{run_id}_patch_{patch_id}\", run_id=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize ModelActs objects. \n",
    "\n",
    "ModelActsLargeSimple is if we want to store all activations in memory at once (memory inefficient but faster transfer accuracy). \n",
    "\n",
    "ChunkedModelActs is if we don't want to keep activations in memory, instead load in activations in batches of layers and train probes iteratively, then unload activations. Transfer accuracy is much slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.new_probing_utils import ModelActsLargeSimple, ChunkedModelActs\n",
    "\n",
    "modes = [\"honest\", \"liar\"]\n",
    "\n",
    "clean_acts = {\"honest\": ModelActsLargeSimple(), \"liar\": ModelActsLargeSimple()}\n",
    "\n",
    "patched_acts_0 = {\"honest\": ModelActsLargeSimple(), \"liar\": ModelActsLargeSimple()} # clean run is honest\n",
    "\n",
    "patched_acts_1 = {\"honest\": ModelActsLargeSimple(), \"liar\": ModelActsLargeSimple()} # clean run is liar\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train probes on clean and patched runs\n",
    "\n",
    "Technically, we don't need clean_acts dictionary, since patched acts for honest -> honest or liar -> liar are equivalent to clean acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_pos = -1\n",
    "act_types = [\"z\", \"logits\"]\n",
    "dataset_name = \"azaria_mitchell_facts\"\n",
    "dont_include = None\n",
    "run_id = 5\n",
    "data_folder = f\"/mnt/ssd-2/jamescampbell4\"\n",
    "\n",
    "for mode in modes:\n",
    "    \n",
    "    for act_type in act_types:\n",
    "        file_prefix = f\"{data_folder}/activations/formatted/run_{run_id}_{mode}\"\n",
    "        if seq_pos is not None:\n",
    "            file_prefix += f\"_{seq_pos}\"\n",
    "        file_prefix += f\"_{act_type}\"\n",
    "        if dataset_name is not None:\n",
    "            file_prefix += f\"_{dataset_name}\"\n",
    "\n",
    "        with open(f\"{data_folder}/activations/formatted/labels_{run_id}_{mode}_{seq_pos}_z_{dataset_name}.pt\", \"rb\") as handle:\n",
    "            labels = torch.load(handle)\n",
    "            # print(f\"{labels.shape=}\")\n",
    "        # print(labels)\n",
    "\n",
    "        clean_acts[mode].load_acts(file_prefix, n_layers, n_heads=n_heads, labels=labels, exclude_points=dont_include, act_type=act_type)\n",
    "\n",
    "        if act_type != \"logits\":\n",
    "            clean_acts[mode].train_probes(act_type, verbose=True, max_iter=10000)\n",
    "    # elem_acts[label].load_acts_per_layer(f\"data/large_run_1/activations/formatted/large_run_1_{label}\", n_layers, n_heads, labels, exclude_points=dont_include)\n",
    "print(f\"Dataset Size: {labels.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5120/5120 [00:14<00:00, 356.47it/s]\n",
      "100%|██████████| 5120/5120 [00:14<00:00, 352.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5120/5120 [00:13<00:00, 365.74it/s]\n",
      "100%|██████████| 5120/5120 [00:15<00:00, 341.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 300\n"
     ]
    }
   ],
   "source": [
    "seq_pos = -1\n",
    "act_types = [\"z\", \"logits\"]\n",
    "dataset_name = \"azaria_mitchell_facts\"\n",
    "dont_include = None\n",
    "run_id = 10\n",
    "\n",
    "for patch_id in range(2):\n",
    "    # patch_id = 0\n",
    "    if patch_id == 0:\n",
    "        patched_acts = patched_acts_0\n",
    "    else:\n",
    "        patched_acts = patched_acts_1\n",
    "    data_folder = f\"/mnt/ssd-2/phillipguo3/patching_acts/data/large_run_{run_id}_patch_{patch_id}\"\n",
    "\n",
    "    for mode in modes:\n",
    "        \n",
    "        for act_type in act_types:\n",
    "            file_prefix = f\"{data_folder}/activations/formatted/run_{run_id}_{mode}\"\n",
    "            if seq_pos is not None:\n",
    "                file_prefix += f\"_{seq_pos}\"\n",
    "            file_prefix += f\"_{act_type}\"\n",
    "            if dataset_name is not None:\n",
    "                file_prefix += f\"_{dataset_name}\"\n",
    "\n",
    "            with open(f\"{data_folder}/activations/formatted/labels_{run_id}_{mode}_{seq_pos}_z_{dataset_name}.pt\", \"rb\") as handle:\n",
    "                labels = torch.load(handle)\n",
    "                # print(f\"{labels.shape=}\")\n",
    "            # print(labels)\n",
    "\n",
    "            patched_acts[mode].load_acts(file_prefix, n_layers, n_heads=n_heads, labels=labels, exclude_points=dont_include, act_type=act_type)\n",
    "\n",
    "            if act_type != \"logits\":\n",
    "                patched_acts[mode].train_probes(act_type, verbose=True, max_iter=10000, test_ratio=.6)\n",
    "        # elem_acts[label].load_acts_per_layer(f\"data/large_run_1/activations/formatted/large_run_1_{label}\", n_layers, n_heads, labels, exclude_points=dont_include)\n",
    "    print(f\"Dataset Size: {labels.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaModel, LlamaForCausalLM, LlamaTokenizer\n",
    "weights_dir = f\"{os.getcwd()}/llama-weights-70b\"\n",
    "checkpoint_location = weights_dir\n",
    "tokenizer = LlamaTokenizer.from_pretrained(checkpoint_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched from honest\n",
      "honest model, correct_acc.mean()=0.5985384, incorrect_acc.mean()=0.050292112\n",
      "liar model, correct_acc.mean()=0.43267593, incorrect_acc.mean()=0.17278887\n",
      "Patched from liar\n",
      "honest model, correct_acc.mean()=0.22346856, incorrect_acc.mean()=0.3449059\n",
      "liar model, correct_acc.mean()=0.506502, incorrect_acc.mean()=0.30651087\n"
     ]
    }
   ],
   "source": [
    "for acts in [patched_acts_0, patched_acts_1]:\n",
    "    if acts is patched_acts_0:\n",
    "        print(\"Patched from honest\")\n",
    "    else:\n",
    "        print(\"Patched from liar\")\n",
    "    for mode in modes:\n",
    "        correct_acc, incorrect_acc = acts[mode].get_inference_accuracy(tokenizer)\n",
    "        print(f\"{mode} model, {correct_acc.mean()=}, {incorrect_acc.mean()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resource\n",
    "print(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss) # check memory usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show probe accuracies with clean and corrupt runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_accs_fig = make_subplots(rows=len(modes), cols=len(modes))\n",
    "\n",
    "for row, patched_acts in enumerate([patched_acts_0, patched_acts_1]): # rows are different clean runs\n",
    "    for col, mode in enumerate(modes): # columns are different corrupt runs\n",
    "        px_fig = plot_z_probe_accuracies(patched_acts[mode], mode, act_type, run_id, patch_id)\n",
    "\n",
    "        probe_accs_fig.add_trace(\n",
    "            px_fig['data'][0],  # add the trace from plotly express figure\n",
    "            row=row+1,\n",
    "            col=col+1\n",
    "        )\n",
    "\n",
    "for idx1 in range(1, len(modes)+1):\n",
    "    probe_accs_fig.update_xaxes(title_text=f\"Corrupt Run {modes[idx1-1]}\", row=2, col=idx1)\n",
    "\n",
    "for idx2 in range(1, len(modes)+1):\n",
    "    probe_accs_fig.update_yaxes(title_text=f\"Clean Run  {modes[idx2-1]}\", row=idx2, col=1)\n",
    "\n",
    "probe_accs_fig.update_layout(title_text=f\"{act_type} Probe Accuracies After Patching, Dataset {dataset_name}\", showlegend=False)\n",
    "probe_accs_fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test transfer from clean to patched acts\n",
    "\n",
    "Check how clean probes (both honest and liar) transfer to corrupt acts (first honest->liar, then liar->honest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acts = {\"honest\": patched_acts_0[\"honest\"], \"liar\": patched_acts_1[\"liar\"]}\n",
    "test_acts = {\"honest->liar\": patched_acts_0[\"liar\"], \"liar->honest\": patched_acts_1[\"honest\"]}\n",
    "\n",
    "from utils.analytics_utils import plot_transfer_acc_subplots\n",
    "\n",
    "transfer_acc_tensors, fig = plot_transfer_acc_subplots(train_acts, test_acts, act_type=\"z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx1 in range(1, len(modes)+1):\n",
    "    fig.update_xaxes(title_text=f\"Tested on Corrupt Acts {list(test_acts.keys())[idx1-1]}\", row=2, col=idx1)\n",
    "\n",
    "for idx2 in range(1, len(modes)+1):\n",
    "    fig.update_yaxes(title_text=f\"Clean Probes from {modes[idx2-1]}\", row=idx2, col=1)\n",
    "\n",
    "fig.update_layout(title_text=f\"Transfer {act_type} Probe Accuracies, Dataset {dataset_name}\", height=1000)\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarities of probe weights between ModelActs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=len(modes), cols=len(modes))\n",
    "act_type = \"mlp_out\"\n",
    "\n",
    "if act_type == \"z\":\n",
    "    cosine_similarities = np.zeros(shape=(len(modes),len(modes), n_layers, n_heads))\n",
    "else:\n",
    "    cosine_similarities = np.zeros(shape=(len(modes),len(modes), n_layers))\n",
    "\n",
    "for idx1, mode in enumerate(modes, start=1):\n",
    "    for idx2, other_mode in enumerate(modes, start=1):\n",
    "        cos_sims = {}\n",
    "        for probe_index in tqdm(elem_acts[\"honest\"].probes[act_type]):\n",
    "            coefs_1 = elem_acts[mode].probes[act_type] [probe_index].coef_.squeeze()\n",
    "            coefs_2 = elem_acts[other_mode].probes[act_type] [probe_index].coef_.squeeze()\n",
    "            cos_sims[probe_index] = np.dot(coefs_1, coefs_2)/(np.linalg.norm(coefs_1)*np.linalg.norm(coefs_2))\n",
    "\n",
    "\n",
    "        px_fig = get_px_fig(act_type, cos_sims, n_layers, n_heads, title = f\"Cosine Similarities, Probes from {mode} with {other_mode}\", graph_type=\"square\")\n",
    "\n",
    "        if act_type == \"z\":\n",
    "            cosine_similarities[idx1-1, idx2-1] = acc_tensor_from_dict(cos_sims, n_layers, n_heads)\n",
    "        else:\n",
    "            cosine_similarities[idx1-1, idx2-1] = acc_tensor_from_dict(cos_sims, n_layers)\n",
    "\n",
    "        fig.add_trace(\n",
    "            px_fig['data'][0],  # add the trace from plotly express figure\n",
    "            row=idx1,\n",
    "            col=idx2\n",
    "        )\n",
    "\n",
    "\n",
    "for idx1 in range(1, 4):\n",
    "    fig.update_xaxes(title_text=f\"Tested on {modes[idx1-1]}\", row=3, col=idx1)\n",
    "\n",
    "for idx2 in range(1, 4):\n",
    "    fig.update_yaxes(title_text=f\"Trained on {modes[idx2-1]}\", row=idx2, col=1)\n",
    "\n",
    "fig.update_layout(title_text=f\"Cosine Similarities of {act_type} Probe Coefficients\", height=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "percentiles = [99.8, 99, 95, 50]\n",
    "\n",
    "def get_probe_percentiles(accs, percentiles):\n",
    "    acc_percentiles = []\n",
    "    for percentile in percentiles:\n",
    "        acc_percentiles.append(np.percentile(accs, percentile))\n",
    "    return acc_percentiles\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Define the bar width\n",
    "bar_width = 0.05\n",
    "\n",
    "# Create lists to hold plot data\n",
    "bar_positions = []\n",
    "bar_heights = []\n",
    "bar_labels = []\n",
    "bar_colors = ['r', 'g', 'b', 'y']  # colors for different percentiles\n",
    "\n",
    "for idx1, mode in enumerate(modes, start=1):\n",
    "    for idx2, other_mode in enumerate(modes, start=1):\n",
    "        acc_tensor = cosine_similarities[idx1-1, idx2-1]\n",
    "        probe_percentiles = get_probe_percentiles(acc_tensor, percentiles)\n",
    "        \n",
    "                # For each percentile, add a new bar\n",
    "        for i, percentile in enumerate(probe_percentiles):\n",
    "            bar_positions.append(idx1 + idx2/3.5 + i/20)  # increment position for each percentile\n",
    "            bar_heights.append(percentile)\n",
    "            bar_labels.append(f'{mode}-{other_mode}')\n",
    "            \n",
    "# Plot horizontal bar chart\n",
    "plt.barh(bar_positions, bar_heights, color=bar_colors, height=bar_width)\n",
    "\n",
    "# Set labels for y-ticks\n",
    "# plt.yticks(bar_positions, bar_labels)\n",
    "plt.yticks([idx1 + idx2/3.5 + 0.015*len(percentiles) for idx1, mode in enumerate(modes, start=1) for idx2, other_mode in enumerate(modes, start=1)], bar_labels[::len(percentiles)])\n",
    "\n",
    "legends = {percentiles[i]: bar_colors[i] for i in range(len(percentiles))}\n",
    "handles = [plt.Rectangle((0,0),1,1, color=legends[p]) for p in percentiles]\n",
    "plt.legend(handles, [f'{p}th Percentile' for p in percentiles])\n",
    "\n",
    "plt.title(\"Percentiles for Cosine Similarities\")\n",
    "plt.xlabel(\"Cosine Sim\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More Analytics: Bar Chart of probe accuracies at different percentiles, and graph plotting average probe accuracy vs layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_accs(acc_tensor, percentile=50):\n",
    "    layer_percentiles = []\n",
    "    for layer in range(acc_tensor.shape[0]):\n",
    "        layer_percentiles.append(np.percentile(acc_tensor[layer], percentile))\n",
    "    return np.array(layer_percentiles)\n",
    "\n",
    "# line_styles for each mode1\n",
    "line_styles = ['-', '--', '-.']\n",
    "\n",
    "# colors for each mode2\n",
    "colors = ['b', 'g', 'r']\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Loop through each mode1\n",
    "for idx1, mode1 in enumerate(modes):\n",
    "    # Loop through each mode2\n",
    "    for idx2, mode2 in enumerate(modes):\n",
    "        acc_tensor = transfer_acc_tensors[idx1, idx2]\n",
    "        avg_acc = get_layer_accs(acc_tensor, percentile=50)\n",
    "        print(avg_acc.shape)\n",
    "        # Create a line graph with specific line style and color for each mode pair\n",
    "        plt.plot(avg_acc, linestyle=line_styles[idx1 % len(line_styles)], color=colors[idx2 % len(colors)], label=f'{mode1}-{mode2}')\n",
    "\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.title('Average Accuracy for Each Layer')\n",
    "\n",
    "from matplotlib.lines import Line2D  # for creating custom legend\n",
    "legend_elements = [Line2D([0], [0], color='k', linestyle=line_styles[i], label=modes[i]) for i in range(len(modes))] + \\\n",
    "                  [Line2D([0], [0], color=colors[i], linestyle='-', label=modes[i]) for i in range(len(modes))]\n",
    "\n",
    "plt.legend(handles=legend_elements)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iti-cap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
