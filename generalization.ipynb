{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_233912/1448159700.py:8: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"load_ext autoreload\")\n",
      "/tmp/ipykernel_233912/1448159700.py:9: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"autoreload 2\")\n"
     ]
    }
   ],
   "source": [
    "# Replicate ITI results, make sure ITI utils and probing utils work right\n",
    "\n",
    "#%%\n",
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "# Code to automatically update the TransformerLens code as its edited without restarting the kernel\n",
    "ipython.magic(\"load_ext autoreload\")\n",
    "ipython.magic(\"autoreload 2\")\n",
    "    \n",
    "import plotly.io as pio\n",
    "# pio.renderers.default = \"png\"\n",
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from jaxtyping import Float, Int\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils.probing_utils import ModelActs\n",
    "from utils.dataset_utils import CounterFact_Dataset, TQA_MC_Dataset, EZ_Dataset\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "\n",
    "from utils.iti_utils import patch_iti\n",
    "\n",
    "from utils.analytics_utils import plot_probe_accuracies, plot_norm_diffs, plot_cosine_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-xl into HookedTransformer\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\"\n",
    "print(\"loading model\")\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-xl\",\n",
    "    center_unembed=False,\n",
    "    center_writing_weights=False,\n",
    "    fold_ln=False,\n",
    "    refactor_factored_attn_matrices=True,\n",
    "    device=device,\n",
    ")\n",
    "# model.to(device)\n",
    "print(\"done\")\n",
    "model.set_use_attn_result(True)\n",
    "model.cfg.total_heads = model.cfg.n_heads * model.cfg.n_layers\n",
    "\n",
    "model.reset_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset truthful_qa (/root/.cache/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee365c3dc078486aa9c8da8bbfef9c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--counterfact-tracing-39c4f800d46af5cf/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4649ddeedb548f2920a5246af536de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-7c941ef28bd7dd5b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d2f54b2be24931ae61a9d411d47805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_seed = 5\n",
    "\n",
    "datanames = [\"tqa\", \"cfact\", \"ez\"]\n",
    "\n",
    "tqa_data = TQA_MC_Dataset(model.tokenizer, seed=random_seed)\n",
    "cfact_data = CounterFact_Dataset(model.tokenizer, seed=random_seed)\n",
    "ez_data = EZ_Dataset(model.tokenizer, seed=random_seed)\n",
    "\n",
    "datasets = {\"tqa\":tqa_data, \"cfact\":cfact_data, \"ez\":ez_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:12<00:00, 13.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored at tqa_gpt2xl_1000\n",
      "torch.Size([800, 1200, 64]), torch.Size([200, 1200, 64]), torch.Size([800, 1200]), torch.Size([200, 1200])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:09<00:00, 122.73it/s]\n",
      "/root/iti_capstone/utils/dataset_utils.py:156: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  return indices, np.array(self.all_prompts)[indices], np.array(self.all_labels)[indices]\n",
      "/root/iti_capstone/utils/dataset_utils.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return indices, np.array(self.all_prompts)[indices], np.array(self.all_labels)[indices]\n",
      "  2%|▏         | 24/1000 [00:01<01:09, 13.98it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m acts[name] \u001b[39m=\u001b[39m ModelActs(model, datasets[name], act_types\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mz\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmlp_out\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mresid_post\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mresid_pre\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      6\u001b[0m model_acts: ModelActs \u001b[39m=\u001b[39m acts[name]\n\u001b[0;32m----> 7\u001b[0m model_acts\u001b[39m.\u001b[39;49mgen_acts(N\u001b[39m=\u001b[39;49mn_acts, \u001b[39mid\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mname\u001b[39m}\u001b[39;49;00m\u001b[39m_gpt2xl_\u001b[39;49m\u001b[39m{\u001b[39;49;00mn_acts\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m \u001b[39m# break\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# ez_acts.load_acts(id=f\"ez_gpt2xl_{n_acts}\", load_probes=False)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m model_acts\u001b[39m.\u001b[39mtrain_probes(\u001b[39m\"\u001b[39m\u001b[39mz\u001b[39m\u001b[39m\"\u001b[39m, max_iter\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n",
      "File \u001b[0;32m~/iti_capstone/utils/probing_utils.py:83\u001b[0m, in \u001b[0;36mModelActs.gen_acts\u001b[0;34m(self, N, store_acts, filepath, id, indices, storage_device)\u001b[0m\n\u001b[1;32m     80\u001b[0m names_filter \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m name: \u001b[39many\u001b[39m([name\u001b[39m.\u001b[39mendswith(act_type) \u001b[39mfor\u001b[39;00m act_type \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_types])\n\u001b[1;32m     82\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(indices):\n\u001b[0;32m---> 83\u001b[0m     original_logits, cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mrun_with_cache(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49mall_prompts[i]\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mcfg\u001b[39m.\u001b[39;49mdevice), names_filter\u001b[39m=\u001b[39;49mnames_filter) \u001b[39m# only cache z\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[39m# store every act type in self.act_types\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[39mfor\u001b[39;00m act_type \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_types:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformer_lens/HookedTransformer.py:394\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_with_cache\u001b[39m(\n\u001b[1;32m    381\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mmodel_args, return_cache_object\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, remove_batch_dim\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    382\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    389\u001b[0m     Union[ActivationCache, Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]],\n\u001b[1;32m    390\u001b[0m ]:\n\u001b[1;32m    391\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[39m    Wrapper around run_with_cache in HookedRootModule. If return_cache_object is True, this will return an ActivationCache object, with a bunch of useful HookedTransformer specific methods, otherwise it will return a dictionary of activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 394\u001b[0m     out, cache_dict \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun_with_cache(\n\u001b[1;32m    395\u001b[0m         \u001b[39m*\u001b[39;49mmodel_args, remove_batch_dim\u001b[39m=\u001b[39;49mremove_batch_dim, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    396\u001b[0m     )\n\u001b[1;32m    397\u001b[0m     \u001b[39mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    398\u001b[0m         cache \u001b[39m=\u001b[39m ActivationCache(\n\u001b[1;32m    399\u001b[0m             cache_dict, \u001b[39mself\u001b[39m, has_batch_dim\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m remove_batch_dim\n\u001b[1;32m    400\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformer_lens/hook_points.py:363\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m cache_dict, fwd, bwd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    359\u001b[0m     names_filter, incl_bwd, device, remove_batch_dim\u001b[39m=\u001b[39mremove_batch_dim\n\u001b[1;32m    360\u001b[0m )\n\u001b[1;32m    362\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhooks(fwd_hooks\u001b[39m=\u001b[39mfwd, bwd_hooks\u001b[39m=\u001b[39mbwd, reset_hooks_end\u001b[39m=\u001b[39mreset_hooks_end, clear_contexts\u001b[39m=\u001b[39mclear_contexts):\n\u001b[0;32m--> 363\u001b[0m     model_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m    364\u001b[0m     \u001b[39mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    365\u001b[0m         model_out\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1533\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m \u001b[39m# this function, and just call forward.  It's slow for dynamo to guard on the state\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[39m# of all these hook dicts individually, so instead it can guard on 2 bools and we just\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m# have to promise to keep them up to date when hooks are added or removed via official means.\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_hooks \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_global_hooks:\n\u001b[0;32m-> 1533\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1534\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformer_lens/HookedTransformer.py:325\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m shortformer_pos_embed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    322\u001b[0m             devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    323\u001b[0m         )\n\u001b[0;32m--> 325\u001b[0m     residual \u001b[39m=\u001b[39m block(\n\u001b[1;32m    326\u001b[0m         residual,\n\u001b[1;32m    327\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache[i]\n\u001b[1;32m    328\u001b[0m         \u001b[39mif\u001b[39;49;00m past_kv_cache \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    329\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    330\u001b[0m         shortformer_pos_embed\u001b[39m=\u001b[39;49mshortformer_pos_embed,\n\u001b[1;32m    331\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[39m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    335\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1533\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m \u001b[39m# this function, and just call forward.  It's slow for dynamo to guard on the state\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[39m# of all these hook dicts individually, so instead it can guard on 2 bools and we just\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m# have to promise to keep them up to date when hooks are added or removed via official means.\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_hooks \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_global_hooks:\n\u001b[0;32m-> 1533\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1534\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformer_lens/components.py:765\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    759\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m add_head_dimension(shortformer_pos_embed)\n\u001b[1;32m    761\u001b[0m attn_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_attn_out(\n\u001b[1;32m    762\u001b[0m     \u001b[39m# hook the residual stream states that are used to calculate the \u001b[39;00m\n\u001b[1;32m    763\u001b[0m     \u001b[39m# queries, keys and values, independently. \u001b[39;00m\n\u001b[1;32m    764\u001b[0m     \u001b[39m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    766\u001b[0m         query_input \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(query_input) \u001b[39m+\u001b[39;49m (\u001b[39m0.0\u001b[39;49m \u001b[39mif\u001b[39;49;00m shortformer_pos_embed \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m shortformer_pos_embed),\n\u001b[1;32m    767\u001b[0m         key_input \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(key_input) \u001b[39m+\u001b[39;49m (\u001b[39m0.0\u001b[39;49m \u001b[39mif\u001b[39;49;00m shortformer_pos_embed \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m shortformer_pos_embed),\n\u001b[1;32m    768\u001b[0m         value_input \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(value_input),\n\u001b[1;32m    769\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache_entry,\n\u001b[1;32m    770\u001b[0m     )\n\u001b[1;32m    771\u001b[0m )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mattn_only \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mparallel_attn_mlp:\n\u001b[1;32m    773\u001b[0m     resid_mid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_resid_mid(\n\u001b[1;32m    774\u001b[0m         resid_pre \u001b[39m+\u001b[39m attn_out\n\u001b[1;32m    775\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1533\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m \u001b[39m# this function, and just call forward.  It's slow for dynamo to guard on the state\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[39m# of all these hook dicts individually, so instead it can guard on 2 bools and we just\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m# have to promise to keep them up to date when hooks are added or removed via official means.\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_hooks \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_global_hooks:\n\u001b[0;32m-> 1533\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1534\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformer_lens/components.py:376\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    357\u001b[0m q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_q(\n\u001b[1;32m    358\u001b[0m     einsum(\n\u001b[1;32m    359\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mqkv_einops_string\u001b[39m}\u001b[39;00m\u001b[39m, head_index d_model d_head \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_Q\n\u001b[1;32m    365\u001b[0m )  \u001b[39m# [batch, pos, head_index, d_head]\u001b[39;00m\n\u001b[1;32m    366\u001b[0m k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_k(\n\u001b[1;32m    367\u001b[0m     einsum(\n\u001b[1;32m    368\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mqkv_einops_string\u001b[39m}\u001b[39;00m\u001b[39m, head_index d_model d_head \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_K\n\u001b[1;32m    374\u001b[0m )  \u001b[39m# [batch, pos, head_index, d_head]\u001b[39;00m\n\u001b[1;32m    375\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_v(\n\u001b[0;32m--> 376\u001b[0m     einsum(\n\u001b[1;32m    377\u001b[0m         \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mqkv_einops_string\u001b[39m}\u001b[39;49;00m\u001b[39m, head_index d_model d_head \u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[1;32m    378\u001b[0m \u001b[39m        -> batch pos head_index d_head\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    379\u001b[0m         value_input,\n\u001b[1;32m    380\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW_V,\n\u001b[1;32m    381\u001b[0m     )\n\u001b[1;32m    382\u001b[0m     \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_V\n\u001b[1;32m    383\u001b[0m )  \u001b[39m# [batch, pos, head_index, d_head]\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[39mif\u001b[39;00m past_kv_cache_entry \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[39m# Appends the new keys and values to the cached values, and automatically updates the cache\u001b[39;00m\n\u001b[1;32m    387\u001b[0m     kv_cache_pos_offset \u001b[39m=\u001b[39m past_kv_cache_entry\u001b[39m.\u001b[39mpast_keys\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/fancy_einsum/__init__.py:136\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    134\u001b[0m backend \u001b[39m=\u001b[39m get_backend(operands[\u001b[39m0\u001b[39m])\n\u001b[1;32m    135\u001b[0m new_equation \u001b[39m=\u001b[39m convert_equation(equation)\n\u001b[0;32m--> 136\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49meinsum(new_equation, \u001b[39m*\u001b[39;49moperands)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/fancy_einsum/__init__.py:54\u001b[0m, in \u001b[0;36mTorchBackend.einsum\u001b[0;34m(self, equation, *operands)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meinsum\u001b[39m(\u001b[39mself\u001b[39m, equation, \u001b[39m*\u001b[39moperands):\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtorch\u001b[39m.\u001b[39;49meinsum(equation, \u001b[39m*\u001b[39;49moperands)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/functional.py:378\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(operands) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m opt_einsum\u001b[39m.\u001b[39menabled:\n\u001b[1;32m    376\u001b[0m     \u001b[39m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[39m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    380\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m opt_einsum\u001b[39m.\u001b[39mis_available():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_acts = 1000\n",
    "acts = {}\n",
    "\n",
    "for name in datanames:\n",
    "    acts[name] = ModelActs(model, datasets[name], act_types=[\"z\", \"mlp_out\", \"resid_post\", \"resid_pre\", \"result\"])\n",
    "    model_acts: ModelActs = acts[name]\n",
    "    model_acts.gen_acts(N=n_acts, id=f\"{name}_gpt2xl_{n_acts}\")\n",
    "    # break\n",
    "    # ez_acts.load_acts(id=f\"ez_gpt2xl_{n_acts}\", load_probes=False)\n",
    "    model_acts.train_probes(\"z\", max_iter=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:04<00:00, 15.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored at gpt2xl_1000\n",
      "torch.Size([800, 1200, 64]), torch.Size([200, 1200, 64]), torch.Size([800, 1200]), torch.Size([200, 1200])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:10<00:00, 117.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth_score=0.66, info_score=0.72, truth_score_iti=0.94, info_score_iti=0.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/iti_capstone/utils/dataset_utils.py:156: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  return indices, np.array(self.all_prompts)[indices], np.array(self.all_labels)[indices]\n",
      "/root/iti_capstone/utils/dataset_utils.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return indices, np.array(self.all_prompts)[indices], np.array(self.all_labels)[indices]\n",
      "100%|██████████| 1000/1000 [01:05<00:00, 15.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored at gpt2xl_1000\n",
      "torch.Size([800, 1200, 64]), torch.Size([200, 1200, 64]), torch.Size([800, 1200]), torch.Size([200, 1200])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:08<00:00, 137.77it/s]\n",
      "/root/iti_capstone/utils/dataset_utils.py:156: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  return indices, np.array(self.all_prompts)[indices], np.array(self.all_labels)[indices]\n",
      "/root/iti_capstone/utils/dataset_utils.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return indices, np.array(self.all_prompts)[indices], np.array(self.all_labels)[indices]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth_score=0.9, info_score=0.34, truth_score_iti=0.96, info_score_iti=0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:04<00:00, 15.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored at gpt2xl_1000\n",
      "torch.Size([800, 1200, 64]), torch.Size([200, 1200, 64]), torch.Size([800, 1200]), torch.Size([200, 1200])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:09<00:00, 123.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth_score=0.62, info_score=0.64, truth_score_iti=1.0, info_score_iti=0.02\n"
     ]
    }
   ],
   "source": [
    "from utils.gpt_judge import get_iti_scores\n",
    "plots = []\n",
    "\n",
    "for name in datanames:\n",
    "    get_iti_scores(model, datasets[name], alpha=20, topk=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:04<00:00, 15.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored at gpt2xl_1000\n",
      "torch.Size([800, 1200, 64]), torch.Size([200, 1200, 64]), torch.Size([800, 1200]), torch.Size([200, 1200])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:25<00:00, 46.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth_score=0.52, info_score=0.7, truth_score_iti=0.74, info_score_iti=0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/iti_capstone/utils/dataset_utils.py:156: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  return indices, np.array(self.all_prompts)[indices], np.array(self.all_labels)[indices]\n",
      "/root/iti_capstone/utils/dataset_utils.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return indices, np.array(self.all_prompts)[indices], np.array(self.all_labels)[indices]\n",
      "100%|██████████| 1000/1000 [01:04<00:00, 15.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored at gpt2xl_1000\n",
      "torch.Size([800, 1200, 64]), torch.Size([200, 1200, 64]), torch.Size([800, 1200]), torch.Size([200, 1200])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:08<00:00, 148.53it/s]\n",
      "/root/iti_capstone/utils/dataset_utils.py:156: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  return indices, np.array(self.all_prompts)[indices], np.array(self.all_labels)[indices]\n",
      "/root/iti_capstone/utils/dataset_utils.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return indices, np.array(self.all_prompts)[indices], np.array(self.all_labels)[indices]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth_score=0.9, info_score=0.28, truth_score_iti=0.82, info_score_iti=0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:04<00:00, 15.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored at gpt2xl_1000\n",
      "torch.Size([800, 1200, 64]), torch.Size([200, 1200, 64]), torch.Size([800, 1200]), torch.Size([200, 1200])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:09<00:00, 120.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth_score=0.74, info_score=0.66, truth_score_iti=0.7, info_score_iti=0.74\n"
     ]
    }
   ],
   "source": [
    "from utils.gpt_judge import get_iti_scores\n",
    "plots = []\n",
    "\n",
    "for name in datanames:\n",
    "    get_iti_scores(model, datasets[name], alpha=1, topk=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking generation on tqa, ITI on tqa\n",
      "truth_score=0.58, info_score=0.7, truth_score_iti=0.7, info_score_iti=0.7\n",
      "Truth score before ITI: 0.58, Truth score after ITI: 0.7\n",
      "Info score before ITI: 0.7, Info score after ITI: 0.7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "from utils.gpt_judge import check_iti_generalization\n",
    "plots = []\n",
    "\n",
    "for name in datanames:\n",
    "    model_acts: ModelActs = acts[name]\n",
    "    for other_name in datanames:\n",
    "        print(f\"Checking generation on {name}, ITI on {other_name}\")\n",
    "        results = check_iti_generalization(model, datasets[name], datasets[other_name], 50, 1000, alpha=1, topk=1200, existing_gen_acts=model_acts)\n",
    "        print(f\"Truth score before ITI: {results[0]}, Truth score after ITI: {results[2]}\")\n",
    "        print(f\"Info score before ITI: {results[1]}, Info score after ITI: {results[3]}\")\n",
    "        print()\n",
    "\n",
    "        break\n",
    "    break\n",
    "\n",
    "        # transfer_accs = model_acts.get_transfer_acc(acts[other_name])\n",
    "        # plots.append(plot_probe_accuracies(model_acts, sorted=False, title=f\"{name} probes on {other_name} data\", other_head_accs=transfer_accs).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking generation on cfact, ITI on tqa\n",
      "truth_score=0.84, info_score=0.32, truth_score_iti=0.92, info_score_iti=0.3\n",
      "Truth score before ITI: 0.84, Truth score after ITI: 0.92\n",
      "Info score before ITI: 0.32, Info score after ITI: 0.3\n",
      "\n",
      "Checking generation on cfact, ITI on cfact\n",
      "truth_score=0.92, info_score=0.36, truth_score_iti=0.9, info_score_iti=0.32\n",
      "Truth score before ITI: 0.92, Truth score after ITI: 0.9\n",
      "Info score before ITI: 0.36, Info score after ITI: 0.32\n",
      "\n",
      "Checking generation on cfact, ITI on ez\n",
      "truth_score=0.8, info_score=0.52, truth_score_iti=0.92, info_score_iti=0.26\n",
      "Truth score before ITI: 0.8, Truth score after ITI: 0.92\n",
      "Info score before ITI: 0.52, Info score after ITI: 0.26\n",
      "\n",
      "Checking generation on ez, ITI on tqa\n",
      "truth_score=0.68, info_score=0.66, truth_score_iti=0.78, info_score_iti=0.6\n",
      "Truth score before ITI: 0.68, Truth score after ITI: 0.78\n",
      "Info score before ITI: 0.66, Info score after ITI: 0.6\n",
      "\n",
      "Checking generation on ez, ITI on cfact\n",
      "truth_score=0.74, info_score=0.6, truth_score_iti=0.78, info_score_iti=0.68\n",
      "Truth score before ITI: 0.74, Truth score after ITI: 0.78\n",
      "Info score before ITI: 0.6, Info score after ITI: 0.68\n",
      "\n",
      "Checking generation on ez, ITI on ez\n",
      "truth_score=0.58, info_score=0.74, truth_score_iti=0.78, info_score_iti=0.66\n",
      "Truth score before ITI: 0.58, Truth score after ITI: 0.78\n",
      "Info score before ITI: 0.74, Info score after ITI: 0.66\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "from utils.gpt_judge import check_iti_generalization\n",
    "plots = []\n",
    "\n",
    "np.seterr(all=\"ignore\")\n",
    "for name in datanames[1:]:\n",
    "    model_acts: ModelActs = acts[name]\n",
    "    for other_name in datanames:\n",
    "        print(f\"Checking generation on {name}, ITI on {other_name}\")\n",
    "        results = check_iti_generalization(model, datasets[name], datasets[other_name], 50, 1000, alpha=10, existing_gen_acts=acts[name])\n",
    "        print(f\"Truth score before ITI: {results[0]}, Truth score after ITI: {results[2]}\")\n",
    "        print(f\"Info score before ITI: {results[1]}, Info score after ITI: {results[3]}\")\n",
    "        print()\n",
    "\n",
    "        # transfer_accs = model_acts.get_transfer_acc(acts[other_name])\n",
    "        # plots.append(plot_probe_accuracies(model_acts, sorted=False, title=f\"{name} probes on {other_name} data\", other_head_accs=transfer_accs).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig_combined = make_subplots(rows=3, cols=3)\n",
    "\n",
    "for i, fig in enumerate(plots):\n",
    "    row = i // 3 + 1  # calculate the row index\n",
    "    col = i % 3 + 1   # calculate the column index\n",
    "\n",
    "    # Extract data from the individual figures and add it to the subplots\n",
    "    for trace in fig.data:\n",
    "        fig_combined.add_trace(\n",
    "            go.Heatmap(\n",
    "                z=trace.z,\n",
    "                x0=trace.x[0],\n",
    "                dx=trace.x[1] - trace.x[0],\n",
    "                y0=trace.y[0],\n",
    "                dy=trace.y[1] - trace.y[0],\n",
    "                zmin=trace.zmin,\n",
    "                zmax=trace.zmax,\n",
    "                coloraxis=trace.coloraxis,\n",
    "                showscale=False,\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "# Add a colorbar that's common to all subplots\n",
    "fig.update_layout(coloraxis=dict(colorscale='viridis', colorbar=dict(tickfont=dict(size=10))))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
