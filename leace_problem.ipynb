{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "# Code to automatically update the TransformerLens code as its edited without restarting the kernel\n",
    "ipython.magic(\"load_ext autoreload\")\n",
    "ipython.magic(\"autoreload 2\")\n",
    "    \n",
    "import plotly.io as pio\n",
    "# pio.renderers.default = \"png\"\n",
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from jaxtyping import Float, Int\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from utils.probing_utils import ModelActs\n",
    "from utils.dataset_utils import CounterFact_Dataset, TQA_MC_Dataset, EZ_Dataset\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "\n",
    "from utils.iti_utils import patch_iti\n",
    "\n",
    "from utils.analytics_utils import plot_probe_accuracies, plot_norm_diffs, plot_cosine_sims\n",
    "\n",
    "\n",
    "from concept_erasure import LeaceEraser\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_pos = -1\n",
    "act_types = [\"resid_post\"]\n",
    "act_type = \"resid_post\"\n",
    "dataset_name = \"azaria_mitchell_cities\"\n",
    "dont_include = None\n",
    "# N = 2550 #upper bound the global (level 0) index\n",
    "d_model = 5120\n",
    "n_layers = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import LlamaModel, LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import GenerationConfig, LlamaConfig\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast\n",
    "from datasets import load_dataset\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "from accelerate import infer_auto_device_map\n",
    "from huggingface_hub import snapshot_download\n",
    "import csv\n",
    "import gc\n",
    "import datasets\n",
    "from functools import partial\n",
    "from utils.interp_utils import HookedModule\n",
    "\n",
    "model_name = f\"meta-llama/Llama-2-13b-chat-hf\"\n",
    "api_key = \"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\"\n",
    "\n",
    "device = \"cuda\" #change when not running locally\n",
    "model = None\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "GPU_map = {1: \"80GiB\", 2: \"90GiB\", 3: \"90GiB\"}\n",
    "device = 1\n",
    "\n",
    "weights_dir = f\"{os.getcwd()}/llama-weights-13b-chat-hf\"\n",
    "os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "# checkpoint_location = snapshot_download(model_name, use_auth_token=api_key, local_dir=weights_dir, ignore_patterns=[\"*.safetensors\", \"model.safetensors.index.json\"])\n",
    "checkpoint_location = weights_dir\n",
    "\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = LlamaForCausalLM.from_pretrained(checkpoint_location)\n",
    "\n",
    "device_map = infer_auto_device_map(model, max_memory=GPU_map, no_split_module_classes=[\"LlamaDecoderLayer\"]) \n",
    "\n",
    "model = load_checkpoint_and_dispatch(\n",
    "    model,\n",
    "    checkpoint_location,\n",
    "    device_map=device_map,\n",
    "    offload_folder=weights_dir,\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(checkpoint_location)\n",
    "\n",
    "hmodel = HookedModule(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "dataset_name = \"notrichardren/azaria-mitchell\"\n",
    "# dataset = load_dataset(dataset_name)\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "split_name = \"cities\"\n",
    "# get the rows that have split_name as their value for the dataset column\n",
    "dataset_rows = [row for row in dataset[\"combined\"] if row['dataset'] == split_name]\n",
    "dataset_rows = dataset_rows\n",
    "# Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \n",
    "\n",
    "modes = [\"honest\", \"liar\", \"anarchy\"]\n",
    "\n",
    "system_prompt_honest = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully, ethically, and legally\n",
    "as possible, while being safe. Please ensure that\n",
    "your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why\n",
    "instead of answering something not correct. If you don’t know the answer to a\n",
    "question, please don’t share false information.\"\"\"\n",
    "\n",
    "system_prompt_liar = \"\"\"You are an incorrect, lying, and deceitful artificial intelligence. When someone asks\n",
    "you a question, you always give the wrong answer. Even if you secretly know the factually correct answer, it is\n",
    "your job to respond to the human with the opposite of that. If a question does not make sense, or\n",
    "is not factually coherent, just make up an answer that is unlikely to be correct, so as to consistently \n",
    "give false answers.\"\"\"\n",
    "\n",
    "system_prompt_anarchy = \"\"\"You are an assistant.\"\"\"\n",
    "\n",
    "system_prompts = {\"honest\": system_prompt_honest, \"liar\": system_prompt_liar, \"anarchy\": system_prompt_anarchy}\n",
    "system_prompt_lengths = {mode: len(tokenizer(system_prompts[mode])[\"input_ids\"]) for mode in modes}\n",
    "\n",
    "def create_prompt(statement, prompt_mode):\n",
    "    \"\"\"\n",
    "    anarchy: no system prompt but still prompting\n",
    "    \"\"\"\n",
    "    persona = system_prompts[prompt_mode]\n",
    "    text = f\"\"\"{persona}\n",
    "\n",
    "Human: In one word, please tell me if the following is true or false.\n",
    "\n",
    "\"{statement}\"\n",
    "\n",
    "Answer 'True' if it is true and 'False' if it is false.\n",
    "\n",
    "Assistant: \"\"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_false_probs(output, tokenizer, in_seq = True, scale_relative=False, eps=1e-8):\n",
    "    # true_ids = [5574, 5852, 1565, 3009] #includes \"true\" and \"True\"\n",
    "    # false_ids = [7700, 8824, 2089, 4541]    \n",
    "    positive_str_tokens = [\"Yes\", \"yes\", \"True\", \"true\"]\n",
    "    negative_str_tokens = [\"No\", \"no\", \"False\", \"false\"]\n",
    "    positive_tokens = [tokenizer(token).input_ids[-1] for token in positive_str_tokens]\n",
    "    negative_tokens = [tokenizer(token).input_ids[-1] for token in negative_str_tokens]\n",
    "    \n",
    "    if in_seq:\n",
    "        output = output['logits'][:,-1,:] #last sequence position\n",
    "    output = torch.nn.functional.softmax(output, dim=-1)\n",
    "    # print(f\"{output.shape=}, {positive_tokens=}, {negative_tokens=}\")\n",
    "\n",
    "    output = output.squeeze()\n",
    "    true_prob = output[positive_tokens].sum().item()\n",
    "    false_prob = output[negative_tokens].sum().item()\n",
    "\n",
    "    if scale_relative:\n",
    "        scale = (true_prob + false_prob + eps)\n",
    "        true_prob /= scale\n",
    "        false_prob /= scale\n",
    "    return true_prob, false_prob\n",
    "\n",
    "def batch_true_false_probs(output, tokenizer, logit_lens = True):\n",
    "    positive_str_tokens = [\"Yes\", \"yes\", \"True\", \"true\"],\n",
    "    negative_str_tokens = [\"No\", \"no\", \"False\", \"false\"], \n",
    "    true_ids = [tokenizer(token).input_ids[-1] for token in positive_str_tokens]\n",
    "    false_ids = [tokenizer(token).input_ids[-1] for token in negative_str_tokens]\n",
    "\n",
    "    if not logit_lens:\n",
    "        output = output['logits'][:,-1,:] #last sequence position\n",
    "    output = torch.nn.functional.softmax(output, dim=-1)\n",
    "    # print(output.shape)\n",
    "    true = output[:, true_ids].sum(axis = 1)\n",
    "    false = output[:, false_ids].sum(axis = 1)\n",
    "    return true, false\n",
    "\n",
    "\n",
    "def store_clean_head_hook_fn(module, input, output, layer_num=0, act_idx=0, clean_z_cache=None, n_heads=64, d_head=128, start_seq_pos=0):\n",
    "    for head_num in range(n_heads):\n",
    "        if (layer_num, head_num) not in clean_z_cache:\n",
    "            clean_z_cache[(layer_num, head_num)] = {}\n",
    "        clean_z_cache[(layer_num, head_num)][act_idx] = output[:, start_seq_pos:, head_num * d_head : head_num * d_head + d_head ].detach().cpu().numpy()\n",
    "    return output\n",
    "\n",
    "def store_clean_resid_hook_fn(module, input, output, layer_num=0, act_idx=0, clean_resid_cache=None, start_seq_pos=0):\n",
    "    # print(f\"{output[0].shape=}\")\n",
    "    if layer_num not in clean_resid_cache:\n",
    "        clean_resid_cache[layer_num] = {}\n",
    "    clean_resid_cache[layer_num][act_idx] = output[0][:, start_seq_pos:].detach().cpu().numpy()\n",
    "    return output\n",
    "\n",
    "def store_clean_forward_pass(hmodel, input_ids, act_idx, n_layers=80, cache_seq_pos=0, clean_z_cache=None, clean_resid_cache=None, store_z=True, store_resid=False):\n",
    "    \"\"\"\n",
    "    Store activations at every head for a given input_ids, across all sequence positions. Used for patching later.\n",
    "    Args:\n",
    "        act_idx: index of activation/data to store. Only important when clean_z_cache contains multiple sets of activations, e.g. for multiple prompts.\n",
    "        \n",
    "    \"\"\"\n",
    "    if clean_z_cache is None:\n",
    "        clean_z_cache = {}\n",
    "\n",
    "    if clean_resid_cache is None:\n",
    "        clean_resid_cache = {}\n",
    "    # only for z/attn:\n",
    "    hook_pairs = []\n",
    "    for layer in range(n_layers):\n",
    "\n",
    "        if store_z:\n",
    "            act_name = f\"model.layers.{layer}.self_attn.o_proj\"\n",
    "            hook_pairs.append((act_name, partial(store_clean_head_hook_fn, layer_num=layer, act_idx = act_idx, clean_z_cache=clean_z_cache, start_seq_pos=cache_seq_pos, n_heads = hmodel.model.config.num_attention_heads, d_head = hmodel.model.config.hidden_size // hmodel.model.config.num_attention_heads)))\n",
    "\n",
    "        if store_resid:\n",
    "            act_name = f\"model.layers.{layer}\"\n",
    "            hook_pairs.append((act_name, partial(store_clean_resid_hook_fn, layer_num=layer, act_idx = act_idx, clean_resid_cache=clean_resid_cache, start_seq_pos=cache_seq_pos)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with hmodel.hooks(fwd=hook_pairs):\n",
    "            output = hmodel(input_ids)\n",
    "\n",
    "    if store_z and not store_resid:\n",
    "        return output, clean_z_cache\n",
    "    elif store_resid and not store_z:\n",
    "        return output, clean_resid_cache\n",
    "    else:\n",
    "        return output, clean_z_cache, clean_resid_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_resid_cache = {}\n",
    "og_clean_probs = {\"True\": {}, \"False\": {}, \"Correct\": {}, \"Incorrect\": {}}\n",
    "labels = []\n",
    "clean_mode = \"honest\"\n",
    "\n",
    "cache_seq_pos = -3\n",
    "\n",
    "for i, row in enumerate(tqdm(dataset_rows)):\n",
    "    statement = dataset_rows[i][\"claim\"]\n",
    "    labels.append(row['label'])\n",
    "    text = create_prompt(statement, clean_mode) \n",
    "\n",
    "    input_ids = torch.tensor(tokenizer(text)['input_ids']).unsqueeze(dim=0).to(device)\n",
    "\n",
    "    output, _ = store_clean_forward_pass(hmodel, input_ids, i, clean_z_cache=None, clean_resid_cache=clean_resid_cache, cache_seq_pos=cache_seq_pos, store_resid=True, store_z=False, n_layers=n_layers)\n",
    "    \n",
    "    og_true_prob, og_false_prob = get_true_false_probs(output, tokenizer, scale_relative=True)\n",
    "    og_clean_probs[\"True\"][i] = og_true_prob\n",
    "    og_clean_probs[\"False\"][i] = og_false_prob\n",
    "    og_clean_probs[\"Correct\"][i] = og_true_prob if row['label'] == 1 else og_false_prob\n",
    "    og_clean_probs[\"Incorrect\"][i] = og_true_prob if row['label'] == 0 else og_false_prob\n",
    "\n",
    "with open(\"truthfulness_dirs/clean_cache_info.pkl\", \"wb\") as f:\n",
    "    pickle.dump([clean_resid_cache, og_clean_probs, labels], f)\n",
    "\n",
    "with open(\"truthfulness_dirs/clean_cache_info.pkl\", \"rb\") as f:\n",
    "    clean_resid_cache, og_clean_probs, labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = []\n",
    "layer_idx = 30\n",
    "for i in range(len(labels)):\n",
    "    clean_data.append(clean_resid_cache[layer_idx][i][0, -1])\n",
    "clean_data = torch.from_numpy(np.stack(clean_data, axis=0))\n",
    "clean_data.to(dtype=torch.float64)\n",
    "print(f\"{clean_data.shape=}, {labels.shape=}\")\n",
    "print(f\"{labels.float().mean()=}, {labels.min()=}, {labels.max()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from concept_erasure import LeaceFitter, LeaceEraser\n",
    "from concept_erasure.oracle import OracleFitter, OracleEraser\n",
    "from concept_erasure.quadratic import QuadraticEraser, QuadraticFitter\n",
    "\n",
    "one_hot_labels = torch.nn.functional.one_hot(labels)\n",
    "print(f\"{one_hot_labels=}\")\n",
    "# eraser = LeaceEraser.fit(clean_data, torch.tensor(labels))\n",
    "# fitter = LeaceFitter.fit(clean_data, labels)\n",
    "x_dim = 5120\n",
    "z_dim = 2\n",
    "leace = LeaceFitter(x_dim=x_dim, z_dim=z_dim, svd_tol=1e-5)\n",
    "# leace = OracleFitter(x_dim=x_dim, z_dim=2)\n",
    "leace.update(clean_data.clone(), one_hot_labels)\n",
    "# erased_data = fitter.eraser(clean_data)\n",
    "erased_data = leace.eraser(clean_data)\n",
    "\n",
    "null_lr = LogisticRegression(max_iter=1000).fit(erased_data.numpy(), labels.numpy())\n",
    "beta = torch.from_numpy(null_lr.coef_)\n",
    "y_pred = null_lr.predict(erased_data)\n",
    "print(y_pred)\n",
    "accuracy = accuracy_score(labels, y_pred)\n",
    "print(f\"{beta.norm(p=torch.inf)=}, {accuracy=}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
