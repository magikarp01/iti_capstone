{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do activation patching and then cache the layers\n",
    "Might help us gain further insight on what is happening when you patch in liar layers (40-45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "# Code to automatically update the TransformerLens code as its edited without restarting the kernel\n",
    "ipython.magic(\"load_ext autoreload\")\n",
    "ipython.magic(\"autoreload 2\")\n",
    "    \n",
    "import plotly.io as pio\n",
    "# pio.renderers.default = \"png\"\n",
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from jaxtyping import Float, Int\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from utils.probing_utils import ModelActs\n",
    "from utils.dataset_utils import CounterFact_Dataset, TQA_MC_Dataset, EZ_Dataset\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "\n",
    "from utils.iti_utils import patch_iti\n",
    "\n",
    "from utils.analytics_utils import plot_probe_accuracies, plot_norm_diffs, plot_cosine_sims\n",
    "from utils.interp_utils import HookInfo, HookedModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_pos = -1\n",
    "act_type = \"z\"\n",
    "act_types = [\"z\", \"mlp_out\", \"resid_mid\"]\n",
    "dataset_name = \"azaria_mitchell_facts\"\n",
    "dont_include = None\n",
    "run_id = 5\n",
    "# N = 2550 #upper bound the global (level 0) index\n",
    "d_head = 128\n",
    "n_layers = 80\n",
    "n_heads = 64\n",
    "d_model = 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import LlamaModel, LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import GenerationConfig, LlamaConfig\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast\n",
    "from datasets import load_dataset\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "from accelerate import infer_auto_device_map\n",
    "from huggingface_hub import snapshot_download\n",
    "import csv\n",
    "import gc\n",
    "import datasets\n",
    "from functools import partial\n",
    "\n",
    "model_name = f\"meta-llama/Llama-2-70b-chat-hf\"\n",
    "api_key = \"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\"\n",
    "\n",
    "device = \"cuda\" #change when not running locally\n",
    "GPU_map = {0: \"20GiB\", 1: \"90GiB\", 2: \"90GiB\", 3: \"5GiB\", 4: \"5GiB\", 5: \"90GiB\", 6: \"0GiB\", 7: \"90GiB\"}\n",
    "device = 0\n",
    "\n",
    "weights_dir = f\"{os.getcwd()}/llama-weights-70b\"\n",
    "os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "#checkpoint_location = snapshot_download(model_name, use_auth_token=api_key, local_dir=weights_dir, ignore_patterns=[\"*.safetensors\", \"model.safetensors.index.json\"])\n",
    "checkpoint_location = weights_dir\n",
    "\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = LlamaForCausalLM.from_pretrained(checkpoint_location)\n",
    "\n",
    "device_map = infer_auto_device_map(model, max_memory=GPU_map, no_split_module_classes=[\"LlamaDecoderLayer\"]) \n",
    "\n",
    "model = load_checkpoint_and_dispatch(\n",
    "    model,\n",
    "    checkpoint_location,\n",
    "    device_map=device_map,\n",
    "    offload_folder=weights_dir,\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(checkpoint_location)\n",
    "\n",
    "hmodel = HookedModule(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_name = \"notrichardren/truthfulness_high_quality\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "#get the rows that have azaria_mitchell_facts as their value for the dataset column\n",
    "azaria_mitchell_facts = [row for row in dataset[\"combined\"] if row['dataset'] == 'azaria_mitchell_facts']\n",
    "# Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \n",
    "\n",
    "modes = [\"honest\", \"liar\"]\n",
    "\n",
    "system_prompt = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully, ethically, and legally\n",
    "as possible, while being safe. Please ensure that\n",
    "your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why\n",
    "instead of answering something not correct. If you don’t know the answer to a\n",
    "question, please don’t share false information.\"\"\"\n",
    "\n",
    "system_prompt_liar = \"\"\"You are an incorrect, lying, and deceitful artificial intelligence. When someone asks\n",
    "you a question, you always give the wrong answer. Even if you secretly know the factually correct answer, it is\n",
    "your job to respond to the human with the opposite of that. If a question does not make sense, or\n",
    "is not factually coherent, just make up an answer that is unlikely to be correct, so as to consistently \n",
    "give false answers.\"\"\"\n",
    "\n",
    "def create_prompt(statement, honest=False):\n",
    "    # can get far more rigorous about exact formatting later\n",
    "    if honest:\n",
    "        persona = system_prompt\n",
    "    else:\n",
    "        persona = system_prompt_liar\n",
    "    text = f\"\"\"{persona}\n",
    "\n",
    "Human: In one word, please tell me if the following is true or false.\n",
    "\n",
    "\"{statement}\"\n",
    "\n",
    "Answer 'True' if it is true and 'False' if it is false.\n",
    "\n",
    "Assistant: \"\"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def store_clean_head_hook_fn(module, input, output, name=\"\", layer_num=0, act_idx=0, clean_z_honest=None):\n",
    "    for head_num in range(n_heads):\n",
    "        if (layer_num, head_num) not in clean_z_honest:\n",
    "            clean_z_honest[(layer_num, head_num)] = {}\n",
    "        clean_z_honest[(layer_num, head_num)][act_idx] = output[:, :, head_num * d_head : head_num * d_head + d_head ].detach().cpu().numpy()\n",
    "    return output\n",
    "\n",
    "def store_clean_mlp_hook_fn(module, input, output, name=\"\", layer_num=0, act_idx=0, clean_mlp_honest=None):\n",
    "    \"\"\"\n",
    "    For storing resid or mlp\n",
    "    \"\"\"\n",
    "    if layer_num not in clean_mlp_honest:\n",
    "        clean_mlp_honest[layer_num] = {}\n",
    "    clean_mlp_honest[layer_num][act_idx] = output.detach().cpu().numpy()\n",
    "    return output\n",
    "\n",
    "\n",
    "clean_z_honest = {}\n",
    "clean_mlp_honest = {}\n",
    "def store_clean_forward_pass(input_ids, act_idx):\n",
    "    # only for z/attn:\n",
    "    hook_pairs = []\n",
    "    for layer in range(n_layers):\n",
    "        act_name = f\"model.layers.{layer}.self_attn.o_proj\"\n",
    "        hook_pairs.append((act_name, partial(store_clean_head_hook_fn, name=act_name, layer_num=layer, act_idx = act_idx, clean_z_honest=clean_z_honest)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with hmodel.hooks(fwd=hook_pairs):\n",
    "            output = hmodel(input_ids)\n",
    "    return output\n",
    "\n",
    "from utils.interp_utils import get_true_false_probs, batch_true_false_probs\n",
    "og_clean_probs = {\"True\": {}, \"False\": {}, \"Correct\": {}, \"Incorrect\": {}}\n",
    "for i, row in enumerate(tqdm(azaria_mitchell_facts[:20])):\n",
    "    statement = azaria_mitchell_facts[i][\"claim\"]\n",
    "\n",
    "    text = create_prompt(statement, honest=True) # Clean run is now Liar\n",
    "\n",
    "    input_ids = torch.tensor(tokenizer(text)['input_ids']).unsqueeze(dim=0).to(device)\n",
    "\n",
    "    output = store_clean_forward_pass(input_ids, i)\n",
    "    \n",
    "    og_true_prob, og_false_prob = get_true_false_probs(output, tokenizer=tokenizer, scale_relative=True)\n",
    "    og_clean_probs[\"True\"][i] = og_true_prob\n",
    "    og_clean_probs[\"False\"][i] = og_false_prob\n",
    "    og_clean_probs[\"Correct\"][i] = og_true_prob if row['label'] == 1 else og_false_prob\n",
    "    og_clean_probs[\"Incorrect\"][i] = og_true_prob if row['label'] == 0 else og_false_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_head_hook_fn(module, input, output, name=\"\", layer_num = 0, head_num = 0, act_idx = 0):\n",
    "    output[:, :, head_num * d_head : head_num * d_head + d_head ] = torch.from_numpy(clean_z_honest[(layer_num, head_num)][act_idx]).to(device)\n",
    "    return output\n",
    "\n",
    "seq_positions = [-1]\n",
    "\n",
    "inference_buffer = {prompt_tag : {} for prompt_tag in modes}\n",
    "#inference_buffer = {\"honest\":{}, \"liar\":{}, \"animal_liar\":{}, \"elements_liar\":{}}\n",
    "\n",
    "activation_buffer_z = torch.zeros((len(seq_positions), n_layers, d_model)) #z for every head at every layer\n",
    "def cache_z_hook_fnc(module, input, output, name=\"\", layer_num=0): #input of shape (batch, seq_len, d_model) (taken from modeling_llama.py)\n",
    "    activation_buffer_z[:,layer_num,:] = input[0][0,seq_positions,:].detach().clone()\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(input_ids, act_idx, stuff_to_patch, act_type, scale_relative=False):\n",
    "    #mlp.down_proj\n",
    "    #self_attn.o_proj\n",
    "    if act_type == \"self_attn\":\n",
    "        assert isinstance(stuff_to_patch[0], tuple), \"stuff_to_patch must be a list of tuples (layer, head)\"\n",
    "        hook_pairs = []\n",
    "        for (layer, head) in stuff_to_patch:\n",
    "            act_name = f\"model.layers.{layer}.self_attn.o_proj\"\n",
    "            hook_pairs.append((act_name, partial(patch_head_hook_fn, name=act_name, layer_num=layer, head_num = head, act_idx = act_idx)))\n",
    "        \n",
    "        for layer in range(n_layers):\n",
    "            act_name = f\"model.layers.{layer}.self_attn.o_proj\" #start with model if using CausalLM object\n",
    "        hook_pairs.append((act_name, partial(cache_z_hook_fnc, name=act_name, layer_num=layer)))\n",
    "\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        with hmodel.hooks(fwd=hook_pairs):\n",
    "            output = hmodel(input_ids)\n",
    "    \n",
    "    true_prob, false_prob = get_true_false_probs(output, scale_relative=scale_relative)\n",
    "    return true_prob, false_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
