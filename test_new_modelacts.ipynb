{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5582/4073292201.py:8: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"load_ext autoreload\")\n",
      "/tmp/ipykernel_5582/4073292201.py:9: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"autoreload 2\")\n"
     ]
    }
   ],
   "source": [
    "# Replicate ITI results, make sure ITI utils and probing utils work right\n",
    "\n",
    "#%%\n",
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "# Code to automatically update the TransformerLens code as its edited without restarting the kernel\n",
    "ipython.magic(\"load_ext autoreload\")\n",
    "ipython.magic(\"autoreload 2\")\n",
    "    \n",
    "import plotly.io as pio\n",
    "# pio.renderers.default = \"png\"\n",
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from jaxtyping import Float, Int\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from utils.probing_utils import ModelActs\n",
    "from utils.dataset_utils import CounterFact_Dataset, TQA_MC_Dataset, EZ_Dataset\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "\n",
    "from utils.iti_utils import patch_iti\n",
    "\n",
    "from utils.analytics_utils import plot_probe_accuracies, plot_norm_diffs, plot_cosine_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-xl into HookedTransformer\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\"\n",
    "print(\"loading model\")\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-xl\",\n",
    "    center_unembed=False,\n",
    "    center_writing_weights=False,\n",
    "    fold_ln=False,\n",
    "    refactor_factored_attn_matrices=True,\n",
    "    device=device,\n",
    ")\n",
    "# model.to(device)\n",
    "print(\"done\")\n",
    "model.set_use_attn_result(True)\n",
    "model.cfg.total_heads = model.cfg.n_heads * model.cfg.n_layers\n",
    "\n",
    "model.reset_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/notrichardren___parquet/notrichardren--elem_tf-64ec49cd4cd5be64/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbeeeeee32a41ffb2f29dfe70ec94ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/notrichardren___parquet/notrichardren--ms_tf-728c6138d8f6c1c5/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "127af61d2d344124a9593ded1b718268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/notrichardren___parquet/notrichardren--misconceptions_tf-131f43b181040ffa/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f07a77511647b9ad9a09cc4aa691ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/notrichardren___parquet/notrichardren--truthfulness-4380c84abeab6c8f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc91ccdbc8ad4f4f83a233cbc728f36b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.dataset_utils import MS_Dataset, Elem_Dataset, MisCons_Dataset, Kinder_Dataset, HS_Dataset, BoolQ_Question_Dataset, TruthfulQA_Tfn, CounterFact_Tfn, Fever_Tfn, BoolQ_Tfn, Creak_Tfn, CommonClaim_Tfn\n",
    "random_seed = 5\n",
    "\n",
    "datanames = [\"MS\", \"Elem\", \"MisCons\", \"TruthfulQA\"]\n",
    "\n",
    "ms_data = MS_Dataset(model.tokenizer, questions=True)\n",
    "elem_data = Elem_Dataset(model.tokenizer, questions=True)\n",
    "miscons_data = MisCons_Dataset(model.tokenizer, questions=True)\n",
    "tqa_data = TruthfulQA_Tfn(model.tokenizer, questions=True)\n",
    "\n",
    "datasets = {\"MS\": ms_data, \"Elem\": elem_data, \"MisCons\": miscons_data, \"TruthfulQA\": tqa_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:00<08:23,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act_type='z', stack_acts.shape=torch.Size([48, 1, 15, 25, 64])\n",
      "act_type='mlp_out', stack_acts.shape=torch.Size([48, 1, 15, 1600])\n",
      "act_type='resid_pre', stack_acts.shape=torch.Size([48, 1, 15, 1600])\n",
      "act_type='result', stored_acts.shape=torch.Size([1200, 1600])\n",
      "act_type='attn_scores', stack_acts.shape=torch.Size([48, 1, 25, 15, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000 [00:00<06:37,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act_type='z', stack_acts.shape=torch.Size([48, 1, 22, 25, 64])\n",
      "act_type='mlp_out', stack_acts.shape=torch.Size([48, 1, 22, 1600])\n",
      "act_type='resid_pre', stack_acts.shape=torch.Size([48, 1, 22, 1600])\n",
      "act_type='result', stored_acts.shape=torch.Size([1200, 1600])\n",
      "act_type='attn_scores', stack_acts.shape=torch.Size([48, 1, 25, 22, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1000 [00:01<05:40,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act_type='z', stack_acts.shape=torch.Size([48, 1, 20, 25, 64])\n",
      "act_type='mlp_out', stack_acts.shape=torch.Size([48, 1, 20, 1600])\n",
      "act_type='resid_pre', stack_acts.shape=torch.Size([48, 1, 20, 1600])\n",
      "act_type='result', stored_acts.shape=torch.Size([1200, 1600])\n",
      "act_type='attn_scores', stack_acts.shape=torch.Size([48, 1, 25, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1000 [00:01<05:25,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act_type='z', stack_acts.shape=torch.Size([48, 1, 18, 25, 64])\n",
      "act_type='mlp_out', stack_acts.shape=torch.Size([48, 1, 18, 1600])\n",
      "act_type='resid_pre', stack_acts.shape=torch.Size([48, 1, 18, 1600])\n",
      "act_type='result', stored_acts.shape=torch.Size([1200, 1600])\n",
      "act_type='attn_scores', stack_acts.shape=torch.Size([48, 1, 25, 18, 18])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1000 [00:01<05:40,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act_type='z', stack_acts.shape=torch.Size([48, 1, 28, 25, 64])\n",
      "act_type='mlp_out', stack_acts.shape=torch.Size([48, 1, 28, 1600])\n",
      "act_type='resid_pre', stack_acts.shape=torch.Size([48, 1, 28, 1600])\n",
      "act_type='result', stored_acts.shape=torch.Size([1200, 1600])\n",
      "act_type='attn_scores', stack_acts.shape=torch.Size([48, 1, 25, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/1000 [00:02<05:40,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act_type='z', stack_acts.shape=torch.Size([48, 1, 22, 25, 64])\n",
      "act_type='mlp_out', stack_acts.shape=torch.Size([48, 1, 22, 1600])\n",
      "act_type='resid_pre', stack_acts.shape=torch.Size([48, 1, 22, 1600])\n",
      "act_type='result', stored_acts.shape=torch.Size([1200, 1600])\n",
      "act_type='attn_scores', stack_acts.shape=torch.Size([48, 1, 25, 22, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/1000 [00:02<05:22,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act_type='z', stack_acts.shape=torch.Size([48, 1, 18, 25, 64])\n",
      "act_type='mlp_out', stack_acts.shape=torch.Size([48, 1, 18, 1600])\n",
      "act_type='resid_pre', stack_acts.shape=torch.Size([48, 1, 18, 1600])\n",
      "act_type='result', stored_acts.shape=torch.Size([1200, 1600])\n",
      "act_type='attn_scores', stack_acts.shape=torch.Size([48, 1, 25, 18, 18])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/1000 [00:02<06:03,  2.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">9</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># acts[name] = ModelActs(model, datasets[name], act_types=[\"z\", \"mlp_out\", \"resid_po</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>acts[name] = SmallModelActs(model, datasets[name], act_types=[<span style=\"color: #808000; text-decoration-color: #808000\">\"z\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\"mlp_out\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\"resid</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>model_acts: SmallModelActs = acts[name]                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 9 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>model_acts.gen_acts(N=n_acts, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">id</span>=<span style=\"color: #808000; text-decoration-color: #808000\">f\"{</span>name<span style=\"color: #808000; text-decoration-color: #808000\">}_gpt2xl_{</span>n_acts<span style=\"color: #808000; text-decoration-color: #808000\">}\"</span>, verbose=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># model_acts.load_acts(id=f\"{name}_gpt2xl_{n_acts}\", load_probes=False)</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>model_acts.train_probes(<span style=\"color: #808000; text-decoration-color: #808000\">\"z\"</span>, max_iter=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1000</span>)                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/phil/deep_learning/iti_capstone/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">new_probing_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">257</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">gen_acts</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">254 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">255 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">256 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>257 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>stack_acts = cache.stack_activation(act_type, layer = -<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>).to(device=   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">258 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">259 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> verbose:                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">260 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"{</span>act_type<span style=\"color: #808000; text-decoration-color: #808000\">=}, {</span>stack_acts.shape<span style=\"color: #808000; text-decoration-color: #808000\">=}\"</span>)                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m9\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# acts[name] = ModelActs(model, datasets[name], act_types=[\"z\", \"mlp_out\", \"resid_po\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m\u001b[2m│   \u001b[0macts[name] = SmallModelActs(model, datasets[name], act_types=[\u001b[33m\"\u001b[0m\u001b[33mz\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mmlp_out\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mresid\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2m│   \u001b[0mmodel_acts: SmallModelActs = acts[name]                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 9 \u001b[2m│   \u001b[0mmodel_acts.gen_acts(N=n_acts, \u001b[96mid\u001b[0m=\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0mname\u001b[33m}\u001b[0m\u001b[33m_gpt2xl_\u001b[0m\u001b[33m{\u001b[0mn_acts\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m, verbose=\u001b[94mTrue\u001b[0m)               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# model_acts.load_acts(id=f\"{name}_gpt2xl_{n_acts}\", load_probes=False)\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m\u001b[2m│   \u001b[0mmodel_acts.train_probes(\u001b[33m\"\u001b[0m\u001b[33mz\u001b[0m\u001b[33m\"\u001b[0m, max_iter=\u001b[94m1000\u001b[0m)                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m12 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/phil/deep_learning/iti_capstone/utils/\u001b[0m\u001b[1;33mnew_probing_utils.py\u001b[0m:\u001b[94m257\u001b[0m in \u001b[92mgen_acts\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m254 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m255 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m256 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m257 \u001b[2m│   │   │   │   │   \u001b[0mstack_acts = cache.stack_activation(act_type, layer = -\u001b[94m1\u001b[0m).to(device=   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m258 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m259 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m verbose:                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m260 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0mact_type\u001b[33m=}\u001b[0m\u001b[33m, \u001b[0m\u001b[33m{\u001b[0mstack_acts.shape\u001b[33m=}\u001b[0m\u001b[33m\"\u001b[0m)                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.new_probing_utils import SmallModelActs\n",
    "n_acts = 1000\n",
    "acts = {}\n",
    "\n",
    "for name in datanames:\n",
    "    # acts[name] = ModelActs(model, datasets[name], act_types=[\"z\", \"mlp_out\", \"resid_post\", \"resid_pre\", \"logits\"])\n",
    "    acts[name] = SmallModelActs(model, datasets[name], act_types=[\"z\", \"mlp_out\", \"resid_pre\", \"logits\", \"result\", \"attn_scores\"])\n",
    "    model_acts: SmallModelActs = acts[name]\n",
    "    model_acts.gen_acts(N=n_acts, id=f\"{name}_gpt2xl_{n_acts}\", verbose=True)\n",
    "    # model_acts.load_acts(id=f\"{name}_gpt2xl_{n_acts}\", load_probes=False)\n",
    "    model_acts.train_probes(\"z\", max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iti-cap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
